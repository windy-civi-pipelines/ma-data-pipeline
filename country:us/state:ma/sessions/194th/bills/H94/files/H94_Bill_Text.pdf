HOUSE DOCKET, NO. 396 FILED ON: 1/8/2025
HOUSE . . . . . . . . . . . . . . . No. 94
The Commonwealth of Massachusetts
_________________
PRESENTED BY:
Francisco E. Paulino
_________________
To the Honorable Senate and House of Representatives of the Commonwealth of Massachusetts in General
Court assembled:
The undersigned legislators and/or citizens respectfully petition for the adoption of the accompanying bill:
An Act to ensure accountability and transparency in artificial intelligence systems.
_______________
PETITION OF:
NAME: DISTRICT/ADDRESS: DATE ADDED:
Francisco E. Paulino 16th Essex 1/8/2025
1 of 1

HOUSE DOCKET, NO. 396 FILED ON: 1/8/2025
HOUSE . . . . . . . . . . . . . . . No. 94
By Representative Paulino of Methuen, a petition (accompanied by bill, House, No. 94) of
Francisco E. Paulino for legislation to ensure accountability and transparency in artificial
intelligence systems. Advanced Information Technology, the Internet and Cybersecurity.
The Commonwealth of Massachusetts
_______________
In the One Hundred and Ninety-Fourth General Court
(2025-2026)
_______________
An Act to ensure accountability and transparency in artificial intelligence systems.
Be it enacted by the Senate and House of Representatives in General Court assembled, and by the authority
of the same, as follows:
1 SECTION 1. Chapter 93M of the General Laws is hereby established as follows:
2 CHAPTER 93M: Artificial Intelligence Accountability and Consumer Protection
3 Section 1. Definitions
4 For the purposes of this Chapter:
5 (1) Algorithmic Discrimination: Differential treatment or impact resulting from an
6 artificial intelligence system that disadvantages individuals or groups based on actual or
7 perceived age, race, ethnicity, gender, disability, national origin, religion, genetic information,
8 reproductive health, veteran status, or any protected classification under Massachusetts or federal
9 law.
1 of 7

10 (2) Artificial Intelligence System: Any machine-based system that processes inputs to
11 generate outputs, including content, decisions, predictions, or recommendations, that influence
12 physical or virtual environments.
13 (3) High-Risk Artificial Intelligence System: AI systems that materially influence
14 consequential decisions, including but not limited to:
15 (a) Education opportunities;
16 (b) Employment decisions;
17 (c) Financial or lending services;
18 (d) Housing access;
19 (e) Healthcare services;
20 (f) Insurance decisions;
21 (g) Legal or government services.
22 (4) Consequential Decision: A decision with significant legal, financial, or personal
23 implications for a consumer, such as denying housing, employment, or healthcare. For clarity,
24 material influence refers to decisions where AI systems determine or heavily weigh inputs that
25 directly affect such outcomes.
26 (5) Developer: An entity or individual developing, modifying, or making AI systems
27 available in Massachusetts.
2 of 7

28 (6) Deployer: An entity using AI systems to make decisions impacting consumers in
29 Massachusetts.
30 (7) Consumer: A resident of the Commonwealth of Massachusetts.
31 Section 2. Developer Responsibilities
32 (a) Duty of Care: Developers must use reasonable care to identify, mitigate, and disclose
33 risks of algorithmic discrimination.
34 (b) Documentation Requirements: Developers must provide deployers with: (1) A
35 summary of intended and foreseeable uses of the AI system; (2) Known limitations and risks,
36 including algorithmic discrimination; (3) Information on the datasets used for training, including
37 measures taken to mitigate biases.
38 (c) Disclosure of Risks: Developers must notify the Attorney General and deployers of
39 any known or foreseeable risks of discrimination within 90 days of discovery.
40 (d) Public Statement: Developers must publish a plain-language summary on their
41 website, detailing: (1) Types of AI systems they develop; (2) Measures to mitigate algorithmic
42 discrimination; (3) Contact information for inquiries.
43 Section 3. Deployer Responsibilities
44 (a) Risk Management Policy: Deployers of high-risk AI systems must implement and
45 maintain a risk management program that:
3 of 7

46 (1) Identifies and mitigates known or foreseeable risks of algorithmic discrimination; (2)
47 Aligns with industry standards, such as the National Institute of Standards and Technology
48 (NIST) AI Risk Management Framework.
49 (b) Impact Assessments:
50 (1) Deployers must complete an annual impact assessment for each high-risk AI system,
51 including: (i) The purpose and intended use of the system; (ii) Data categories used and outputs
52 generated; (iii) Potential risks of discrimination and mitigation measures.
53 (2) Impact assessments must be updated after any substantial modification to the system.
54 State-provided templates for these assessments will be made available to reduce compliance
55 burdens.
56 (c) Consumer Protections: Deployers must:
57 (1) Notify consumers when an AI system materially influences a consequential decision;
58 (2) Provide consumers with: (i) The purpose of the system; (ii) An explanation of how the
59 system influenced the decision; (iii) A process to appeal or correct adverse decisions.
60 (d) Transparency: Deployers must publicly disclose the types of high-risk AI systems in
61 use and their risk mitigation strategies.
62 SECTION 4. Corporate Disclosure Requirements
63 (a) Disclosure of AI Use: Any corporation operating in Massachusetts that uses artificial
64 intelligence systems or related tools to target specific consumer groups or influence behavior
65 must disclose:
4 of 7

66 (1) Purpose of AI Use: The methods, purposes, and contexts in which AI systems are
67 used to identify or target specific classes of individuals; (2) Behavioral Influence: The specific
68 ways in which AI tools are designed to influence consumer behavior; (3) Third-Party
69 Partnerships: Details of any third-party entities involved in the design, deployment, or operation
70 of AI systems used for targeting or behavioral influence. Proprietary information will be
71 safeguarded and exempt from public disclosure under state confidentiality laws.
72 (b) Public Disclosure Requirements: Corporations must make these disclosures: (1)
73 Publicly available on their website in a manner that is easily accessible and comprehensible; (2)
74 Included in terms and conditions provided to consumers prior to significant interaction with an
75 AI system.
76 (c) Consumer Notification: Consumers must be notified when: (1) They are being
77 targeted or influenced by AI systems in a way that materially impacts their decisions; (2)
78 Algorithms are used to determine pricing, eligibility, or access to services.
79 SECTION 5. Exemptions
80 The following entities and circumstances are exempt from certain provisions of this
81 Chapter:
82 (1) Small Businesses: Businesses with fewer than 50 employees that do not use
83 proprietary data to train AI systems.
84 (2) Low-Risk Systems: AI systems performing procedural tasks (e.g., spell-checkers,
85 calculators) or those not influencing consequential decisions.
5 of 7

86 (3) Federal Compliance: Entities subject to equivalent or stricter federal AI regulations,
87 such as those governed by the Federal Trade Commission or Department of Health and Human
88 Services.
89 SECTION 6. Enforcement
90 (a) Attorney General Authority: The Attorney General has exclusive authority to enforce
91 this Chapter. Violations are deemed unfair or deceptive trade practices under Chapter 93A.
92 (b) Affirmative Defense: A developer or deployer may defend against enforcement if:
93 (1) They identify and remedy violations through testing, internal review, or consumer
94 feedback; (2) They demonstrate compliance with recognized AI risk management standards.
95 (c) No Private Right of Action: This Chapter does not create a private right of action for
96 consumers.
97 SECTION 7. Rulemaking Authority
98 The Attorney General may issue rules to: (1) Define documentation and impact
99 assessment requirements (2) Set standards for risk management programs and consumer
100 notifications; (3) Designate recognized AI risk management frameworks.
101 SECTION 8. Public Education Campaign
102 The Attorney General, in collaboration with relevant state agencies, shall establish a
103 public education campaign to inform residents of their rights under this Chapter and to increase
104 awareness of the role of AI in decision-making processes.
105 SECTION 9. Sections 1, 4, 5, and 8 shall take effect 180 days after passage of this Act.
6 of 7

106 SECTION 10. Sections 2, 3, 6, and 7 shall take effect 1 year after passage of this Act.
107 SECTION 11. The Amendment to Chapter 93A shall take effect 180 days after passage
108 of this Act.
7 of 7

[DELETED: :D/A:DAEITDSA Do1x1]
[DELETED: SCSF(aprl]
[DELETED: 0(1g2p3(4c5(6(7(8(9(0(1(2(3i4m5d6(7a]
[DELETED: 8(9M0(1S2(3r4(5s6i7m8(9a0(1w2d3S4(5m]
[DELETED: 6(7A8(9(0(1i2g3(4S5b6(7(8(9s0(1u2S3(4i5m]
[DELETED: 6(7u8w9P0o1s2(3P4I5A6(7t8A9S0T1C2(3p4(5c]
[DELETED: 6(7s8S9S0(1t2(3(4f5(6c7S8T9a0n1S2T3p4a5S]
[DELETED: 6S7S8o]