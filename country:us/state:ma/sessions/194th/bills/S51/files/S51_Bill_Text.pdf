SENATE DOCKET, NO. 1313 FILED ON: 1/16/2025
SENATE . . . . . . . . . . . . . . No. 51
The Commonwealth of Massachusetts
_________________
PRESENTED BY:
John C. Velis
_________________
To the Honorable Senate and House of Representatives of the Commonwealth of Massachusetts in General
Court assembled:
The undersigned legislators and/or citizens respectfully petition for the adoption of the accompanying bill:
An Act relative to social media, algorithm accountability, and transparency.
_______________
PETITION OF:
NAME: DISTRICT/ADDRESS:
John C. Velis Hampden and Hampshire
1 of 1

SENATE DOCKET, NO. 1313 FILED ON: 1/16/2025
SENATE . . . . . . . . . . . . . . No. 51
By Mr. Velis, a petition (accompanied by bill, Senate, No. 51) of John C. Velis for legislation
relative to social media, algorithm accountability, and transparency. Advanced Information
Technology, the Internet and Cybersecurity.
The Commonwealth of Massachusetts
_______________
In the One Hundred and Ninety-Fourth General Court
(2025-2026)
_______________
An Act relative to social media, algorithm accountability, and transparency.
Be it enacted by the Senate and House of Representatives in General Court assembled, and by the authority
of the same, as follows:
1 SECTION 1. Chapter 12 of the General Laws, as so appearing, is hereby amended by
2 inserting after section 35 the following section:-
3 Section 36. (a) As used in this section the following words shall, unless the context
4 clearly requires otherwise, have the following meanings:-
5 “Algorithm”, computational process that uses machine learning, natural language
6 processing, artificial intelligence techniques, or other computational processing techniques of
7 similar or greater complexity and that makes a decision or facilitates human decision-making
8 with respect to users personal information, including to determine the provision of products or
9 services or to rank, order, promote, recommend, amplify or similarly determine the delivery or
10 display of information to an individual. For purposes of this section, an algorithm will refer to
11 recommendation algorithms, also known as engagement-based algorithms, which passively
12 populate a user’s feed or experience with content without any direct action or request by the user.
1 of 8

13 “Children”, consumers under 18 years of age.
14 “Covered platform”, an internet website, online service, online application, or mobile
15 application, including, but not limited to, a social media platform that conducts business in this
16 state or that produces products or services that is accessed by residents and that during the
17 preceding calendar year: (1) controlled or processed the personal information of not less than one
18 hundred thousand consumers, excluding personal information controlled or processed solely for
19 the purpose of completing a payment transaction; or (2) controlled or processed the personal
20 information of not less than twenty-five thousand consumers and derived more than twenty-five
21 per cent of their gross revenue from the sale of personal data.
22 “Consumer”, a natural person who is a Massachusetts resident, however identified,
23 including by any unique identifier.
24 “Independent third-party auditor”, auditing organization that has no affiliation with a
25 covered platform as defined by this section.
26 “Likely to be accessed”, reasonable expectation, based on the following factors, that a
27 covered platform would be accessed by children: (1) the covered platform is directed to children
28 as defined by the Children’s Online Privacy Protection Act (15 U.S.C. Sec. 6501 et seq.); (2) the
29 covered platform is determined based on audience composition where children comprise at least
30 10% of its audience; (3) the covered platform is paid for advertisements on its platform that are
31 marketed to children; (4) the covered platform is substantially similar or the same as a covered
32 platform that satisfies subsection (2); and (5) a significant amount of the audience of the covered
33 platform, 10% or more, is determined, based on internal company research, to be children.
2 of 8

34 "Process" or "processing", any operation or set of operations performed, whether by
35 manual or automated means, on personal information or on sets of personal information, such as
36 the collection, use, storage, disclosure, analysis, deletion or modification of personal
37 information.
38 “Personal information”, information linked or reasonably linkable to an identified or
39 identifiable individual.
40 “Social media platform”, public or semipublic internet-based service or application that
41 has users in Massachusetts and that meets both of the following criteria: (1) a substantial
42 function of the service or application is to connect users and allow users to interact socially with
43 each other within the service or application; provided further, that an internet-based service or
44 application that provides email or direct messaging services shall not be considered to meet this
45 criterion on this function alone; provided further that a service or application that is an internet
46 search engine or website whose primary focus is e-commerce, which would include the buying,
47 selling, or exchange of goods or services over the internet, including business-to-business,
48 business-to-consumer, and consumer-to-consumer transactions shall not be considered to meet
49 this criterion on the basis of that function alone; and (2) the application allows users to: (i)
50 construct a public or semipublic profile for purposes of signing into and using the service or
51 application; (ii) populate a list of other users with whom an individual shares a social connection
52 within the system; and (iii) create or post content viewable by other users, including, but not
53 limited to, on message boards, in chat rooms, or through a landing page or main feed that
54 presents the user with content generated by other users.
3 of 8

55 “Experts in the mental health and public policy fields”, (1) academic experts, health
56 professionals, and members of civil society with expertise in mental health, substance use
57 disorders, and the prevention of harms to minors; (2) representatives in academia and civil
58 society with specific expertise in privacy and civil liberties; (3) parents and youth representation;
59 (4) representatives of the national telecommunications and information administration, the
60 national institute of standards and technology, the federal trade commission, the office of the
61 attorney general of Massachusetts, and the Massachusetts executive office of health and human
62 services; (5) state attorneys general or their designees acting in State or local government; and
63 (6) representatives of communities of socially disadvantaged individuals as defined in section 8
64 of the Small Business Act, 15 U.S.C. 637.
65 (b) There shall be an office of social media transparency and accountability, which shall
66 be supervised and controlled by the office of the attorney general. The office shall receive,
67 review and maintain the reports from covered platforms, to enforce this section, and to adopt
68 regulations to clarify the requirements of this section.
69 (c) Annually before January 1, covered platforms shall register with the office by
70 providing: (i) a registration fee, determined by the office of the attorney general; (ii) the
71 platform’s name; (iii) physical address; (iv) email; and (v) internet address.
72 (d) The office shall compile a list of approved, independent third-party auditors and shall
73 assign independent third-party auditors to conduct algorithm risk audits of covered platforms.
74 Risk audits shall be conducted monthly by third-party auditors, unless specified otherwise by the
75 office. Audits and associated costs shall be paid for by covered platforms. The algorithm risk
76 audits shall focus on harms to children, including but not limited to: (i) mental health disorders
4 of 8

77 including anxiety, depression, eating disorders, substance abuse disorders, and suicidal
78 behaviors; (ii) patterns of use that indicate or encourage addiction-like behaviors; (iii) physical
79 violence, online bullying, and harassment of the minor; (iv) sexual exploitation and abuse; (v)
80 promoting and market of narcotic drugs as defined in section 102 of the Controlled Substances
81 Act, 21 U.S.C. 802, tobacco products, gambling, or alcohol; and (vi) predatory, unfair or
82 deceptive marketing practices, or other financial harms.
83 (e) Annually before January 1, the office shall empanel an Advisory Council of experts in
84 the mental health and public policy fields as defined in this section to review these harms and
85 identify additional ways covered platforms cause harms to children.
86 (f) Annually before July 1, the office shall promulgate regulations based on the
87 cumulation of the potential harms identified by the Advisory Council that update the specific
88 harms that must be examined by the algorithm risk audits required under this section.
89 (g) Beginning on January 1, 2026, covered platforms shall annually submit transparency
90 reports to the office containing, but not limited to: (i) assessment of whether the covered
91 platform is likely to be accessed by children; (ii) description of the covered platform’s
92 commercial interests in use of the platform by children; (iii) number of individuals using the
93 covered platform reasonably believed to be children in the United States, disaggregated by the
94 age ranges of 0-5, 6-9, 10-12, 13-15 and 16-17 years; (iv) median and mean amounts of time
95 spent on the covered platform by children in the United States who have accessed the platform
96 during the reporting year on a daily, weekly and monthly basis, disaggregated by the age ranges
97 of 0-5, 6-9, 10-12, 13-15 and 16-17 years; (v) description of whether and how the covered
98 platform uses system design features to increase, sustain, or extend use of a product or service by
5 of 8

99 users, including automatic playing of media, rewards for time spent and notifications; (vi)
100 description of whether, how and for what purpose the covered platform collects or processes
101 personal information that may cause reasonably foreseeable risk of harm to children; (vii) total
102 number of complaints received regarding, and the prevalence of issues related to, the harms
103 described in section 1, disaggregated by category of harm; (viii) description of the mechanism by
104 which the public may submit complaints, the internal processes for handling complaints, and any
105 automated detection mechanisms for harms to children, including the rate, timeliness, and
106 effectiveness of responses.
107 (h) By January 1, 2027, covered platforms shall submit preliminary reports to the office.
108 The preliminary report must measure the incidence of each of the specific harms identified in
109 subsection (d) that occur on the covered platform. The office must consult with independent
110 third-party auditors and covered platforms to determine what data shall be used to produce the
111 preliminary reports.
112 After a covered platform has submitted a preliminary report, the covered platform may
113 agree that the office will consult with independent third-party auditors and the covered platform
114 to set benchmarks the covered platform must meet to reduce the harms, identified in subsection
115 (d) on its platform as indicated in the preliminary reports required under this section. Upon
116 agreement, each covered platform shall thereafter produce biannual reports containing, but not
117 limited to: (i) steps taken to mitigate harm on its platform, including implementation of any
118 systems used to meet benchmarks; and (ii) measurements indicating the redaction in harm as a
119 result of these systems.
6 of 8

120 In the case the covered platform has failed meet the benchmarks, upon agreement its
121 annual report must also contain: (1) a mitigation plan detailing changes the platform intends to
122 take to ensure future compliance with benchmarks; and (2) a written explanation regarding the
123 reasons the benchmarks were not met.
124 If a covered platform should choose not to consult with independent third-party auditors
125 to set benchmarks it must meet to reduce the harms, identified in subsection (d), on its platform
126 as indicated in the preliminary reports required under this subsection, the attorney general is not
127 precluded from pursuing any other legal remedy available at law to mitigate harms.
128 (i) The records generated by this section shall be subject to chapter 66 of the General
129 Laws and shall be made accessible to the public on the attorney general’s website. However, to
130 the extent any information contained within a report required by this section is trade secret,
131 proprietary or privileged, covered platforms may request such information be redacted from the
132 copy of the report that is obtainable under the public records law and on the attorney general’s
133 website. The office will conduct a confidential, in-camera review of requested redactions to
134 determine whether the information is trade secret, proprietary or privileged information that
135 should not be made accessible for public review. All information from the copy of the report
136 submitted to the office, including redactions, will be maintained by a covered platform in their
137 internal records.
138 (j) A covered platform shall be considered in violation of this section for the following:
139 (i) fails to register with the office; (ii) materially omits or misrepresents required information in a
140 submitted report; or (iii) fails to timely submit a report to the office.
7 of 8

141 (1) A covered platform in violation of this section is subject to an injunction and liable
142 for a civil penalty not to exceed $500,000 per violation, which shall be assessed and recovered in
143 a civil action brought by the attorney general. In assessing the amount of a civil penalty pursuant
144 to this section, the court shall consider whether the covered platform made a reasonable, good
145 faith attempt to comply with the provisions of this section. Any penalties, fees, and expenses
146 recovered in an action brought under this section shall be collected by the office of the attorney
147 general with the intent that they be used to fully offset costs in connection with the enforcement
148 of this section and to promote the positive mental health outcomes of the children of
149 Massachusetts.
150 (k) If any provision of this section, or any application of such provision to any person or
151 circumstance, is held to be unconstitutional, the remainder of this section and the application of
152 this section to any other person or circumstance shall not be affected.
8 of 8

[DELETED: :D/AEITDsH]
[DELETED: SiSc“psws0d1r2p]
[DELETED: 3“4“5a6s7p8h9t0i1p2“3i4“5c6“7c8a9c011m2p3p]
[DELETED: 4"5m6t7i8“9i0“1h2f3e4a5c6s7s8b9t0c1a2w3l4p]
[DELETED: 5“6p7d8s9(0n1a2s3(4o5(6b7r8r9(0p1p2(3a4R5o6a]
[DELETED: 7i8b9v0p1A2d3(4t5i6(7c8h9(0r1p2c3c4a5s6d7o8p]
[DELETED: 9u0d1p2n3d4w5a6e7(8T9s0t1p2A3a4t5(6a7l8s9r]
[DELETED: 0I1a2t3r4I5t6a7p8(9L0t1p2c3w4d5s6s7i8(9(0s]
[DELETED: 1(2f3a4t5f6r7g8o9M0(1c2t]